[pytest]
# Pytest configuration for optimized test execution

# Test discovery
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Markers
markers =
    unit: Unit tests
    integration: Integration tests
    contracts: Contract tests
    contract: Contract test (alias for contracts)
    performance: Performance tests
    security: Security tests
    slow: Slow running tests (deselect with: -m "not slow")
    critical: Critical business logic tests
    flaky: Known flaky tests that need fixes
    benchmark: Performance benchmark tests
    serial: Tests that should run serially to avoid timing/concurrency issues

# Output options
addopts =
    -v
    --strict-markers
    --tb=short
    --maxfail=10
    --durations=10
    --show-capture=no
    --ignore=tests/contracts/test_billing_provider_v3.py
    --ignore=tests/contracts/test_billing_consumer_v3.py

# Coverage options
[tool:pytest]
testpaths = tests
norecursedirs = .git .tox dist build *.egg __pycache__ htmlcov .benchmarks

# Parallel execution
# Use: pytest -n auto
# or: pytest -n 4 (for 4 workers)

# Timeout settings
timeout = 600
timeout_method = thread

# Logging
log_cli = false
log_cli_level = INFO
log_file = tests.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s
log_file_date_format = %Y-%m-%d %H:%M:%S

# Flaky test handling
# Requires: pip install pytest-rerunfailures
[pytest-rerunfailures]
reruns = 3
reruns_delay = 1

# Benchmark settings
# Requires: pip install pytest-benchmark
[benchmark]
min_rounds = 5
min_time = 0.000005
max_time = 1.0
calibration_precision = 10
warmup = yes
warmup_iterations = 100000
