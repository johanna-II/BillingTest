name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run nightly to catch flaky tests
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.12'
  USE_MOCK_SERVER: 'true'
  MOCK_SERVER_PORT: '5000'
  PYTEST_RERUN_FAILURES: '3'  # Retry flaky tests

jobs:
  lint:
    name: Lint and Type Check
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install ruff mypy
    
    - name: Run ruff
      run: ruff check .
    
    - name: Run mypy
      run: mypy libs --ignore-missing-imports

  test:
    name: Test Suite - ${{ matrix.test-type }}
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        test-type: [unit, contracts, comprehensive]
    
    steps:
    # Skip comprehensive tests unless on main branch or tagged with [full-test]
    - name: Check if comprehensive should run
      if: matrix.test-type == 'comprehensive'
      run: |
        if [[ "${{ github.ref }}" != "refs/heads/main" ]] && [[ "${{ github.event.head_commit.message }}" != *"[full-test]"* ]]; then
          echo "Skipping comprehensive tests (only runs on main or with [full-test] tag)"
          exit 0
        fi
    
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: Linux-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          Linux-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-xdist pytest-cov pytest-html pytest-rerunfailures pytest-timeout
    
    - name: Start Mock Server
      # Only for contracts (integration & comprehensive use Docker)
      if: matrix.test-type == 'contracts'
      run: |
        # Start mock server in background
        python -m mock_server.run_server &
        MOCK_PID=$!
        echo "Mock server started with PID: $MOCK_PID"
        
        # Wait for server to be ready (up to 30 seconds)
        for i in {1..30}; do
          if curl -s --max-time 2 http://localhost:5000/health > /dev/null 2>&1; then
            echo "✓ Mock server is ready with rate limit 500 req/sec"
            break
          fi
          if [ $i -eq 30 ]; then
            echo "✗ Mock server failed to start in 30 seconds"
            exit 1
          fi
          echo "Waiting for mock server... ($i/30)"
          sleep 1
        done
      env:
        MOCK_SERVER_RATE_LIMIT: 500
        FLASK_ENV: production
    
    - name: Run Tests with Coverage
      run: |
        # coverage 파일명을 명시적으로 지정
        export COVERAGE_FILE=.coverage.${{ matrix.test-type }}
        
        # 단위 테스트는 병렬 실행 (auto) + 60초 타임아웃
        # 계약 테스트는 순차 실행 + 300초 타임아웃
        # 통합 테스트는 integration-tests-service.yml에서 별도 실행
        if [ "${{ matrix.test-type }}" = "unit" ]; then
          # Unit tests run directly (faster, no mock server needed)
          pytest tests/${{ matrix.test-type }}/ -v --tb=short \
            -n auto \
            --timeout=60 \
            --cov=libs --cov-report=term-missing \
            --cov-report=xml:coverage-${{ matrix.test-type }}.xml \
            --junit-xml=test-results-${{ matrix.test-type }}.xml
        elif [ "${{ matrix.test-type }}" = "contracts" ]; then
          # Contracts run directly (sequential)
          pytest tests/${{ matrix.test-type }}/ -v --tb=short \
            --timeout=300 \
            --cov=libs --cov-report=term-missing \
            --cov-report=xml:coverage-${{ matrix.test-type }}.xml \
            --junit-xml=test-results-${{ matrix.test-type }}.xml \
            --use-mock
        elif [ "${{ matrix.test-type }}" = "comprehensive" ]; then
          # Build and run Mock Server as separate container
          echo "Building Mock Server Docker image..."
          docker build -f Dockerfile.mock -t billing-mock-server:ci .
          
          echo "Starting Mock Server..."
          docker run -d \
            --name mock-server-ci \
            --network host \
            -p 5000:5000 \
            -e FLASK_ENV=production \
            -e FLASK_APP=mock_server.app \
            -e PYTHONUNBUFFERED=1 \
            -e MOCK_SERVER_RATE_LIMIT=500 \
            billing-mock-server:ci
          
          echo "Waiting for Mock Server..."
          for i in {1..60}; do
            if curl -f http://localhost:5000/health 2>/dev/null; then
              echo "✓ Mock server ready"
              break
            fi
            [ $i -eq 60 ] && { echo "✗ Failed to start"; docker logs mock-server-ci; exit 1; }
            sleep 1
          done
          
          echo "Running Comprehensive Tests..."
          pytest tests/integration/test_all_business_combinations.py \
            tests/integration/test_comprehensive_business_logic.py \
            -v --tb=short \
            -n 2 \
            --timeout=600 \
            --timeout-method=thread \
            --reruns=3 \
            --reruns-delay=3 \
            --max-worker-restart=20 \
            --dist=loadfile \
            --junit-xml=test-results-${{ matrix.test-type }}.xml \
            --use-mock \
            || { TEST_EXIT_CODE=$?; echo "::warning::Comprehensive tests had failures"; }
          
          echo "Cleanup..."
          docker logs mock-server-ci | tail -50
          docker stop mock-server-ci || true
          docker rm mock-server-ci || true
          
          exit ${TEST_EXIT_CODE:-0}
        fi
      env:
        PYTEST_TIMEOUT: 600
        COMPOSE_HTTP_TIMEOUT: 600
        USE_MOCK_SERVER: 'true'
        MOCK_SERVER_URL: http://localhost:5000
        MOCK_SERVER_PORT: '5000'
        PYTHONUNBUFFERED: '1'
    
    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: test-results-${{ matrix.test-type }}
        path: |
          test-results-${{ matrix.test-type }}.xml
          coverage-${{ matrix.test-type }}.xml
          .coverage.${{ matrix.test-type }}
          htmlcov/

  performance:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-mock.txt
        pip install pytest-benchmark
    
    - name: Start Mock Server
      run: |
        python -m mock_server.run_server &
        for i in {1..10}; do
          if curl -s http://localhost:5000/health; then
            echo "Mock server is ready with rate limit 500 req/sec"
            break
          fi
          echo "Waiting for mock server..."
          sleep 1
        done
      env:
        MOCK_SERVER_RATE_LIMIT: 500
    
    - name: Run Performance Tests
      run: |
        pytest tests/performance/ -v --tb=short \
          --junit-xml=test-results-performance.xml
      env:
        CI: true
        MOCK_SERVER_PORT: 5000
        MOCK_SERVER_RATE_LIMIT: 500
    
    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: test-results-performance
        path: |
          test-results-performance.xml

  security:
    name: Security Tests
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-mock.txt
    
    - name: Start Mock Server
      run: |
        python -m mock_server.run_server &
        for i in {1..10}; do
          if curl -s http://localhost:5000/health; then
            echo "Mock server is ready with rate limit 50 req/sec"
            break
          fi
          echo "Waiting for mock server..."
          sleep 1
        done
      env:
        MOCK_SERVER_RATE_LIMIT: 50
    
    - name: Run Security Tests
      run: |
        pytest tests/security/ -v --tb=short \
          --junit-xml=test-results-security.xml
      env:
        CI: true
        MOCK_SERVER_PORT: 5000
    
    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: test-results-security
        path: |
          test-results-security.xml

  coverage-check:
    name: Coverage Analysis
    runs-on: ubuntu-latest
    needs: test
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Download coverage reports
      uses: actions/download-artifact@v4
      with:
        pattern: test-results-*
        merge-multiple: true
    
    - name: Install coverage
      run: pip install coverage
    
    - name: Combine coverage reports
      run: |
        # Debug: 다운로드된 파일 확인
        echo "Downloaded files:"
        ls -la
        
        # coverage 파일 찾기 및 combine
        if ls .coverage.* 1> /dev/null 2>&1; then
          coverage combine .coverage.*
          coverage report --fail-under=80
          coverage html -d htmlcov-combined
          coverage xml -o coverage-combined.xml
        else
          echo "No .coverage.* files found. Checking for xml files..."
          # XML 파일들이 있다면 직접 병합
          if ls coverage-*.xml 1> /dev/null 2>&1; then
            echo "Found coverage XML files, but cannot combine them directly."
            echo "Skipping coverage combine step."
            exit 0
          else
            echo "No coverage files found!"
            exit 1
          fi
        fi
    
    - name: Upload to Codecov
      # Using specific commit SHA for security (codecov-action v5.0.7)
      uses: codecov/codecov-action@5c47607acb93fed5485fdbf7232e8a31425f672a  # v5.0.7
      with:
        files: ./coverage-combined.xml
        flags: unittests,contracts
        name: combined-coverage
        fail_ci_if_error: true

  publish-coverage:
    name: Publish Coverage Report
    runs-on: ubuntu-latest
    needs: [lint, test, coverage-check]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    steps:
    - uses: actions/checkout@v4
    
    - name: Download coverage reports
      uses: actions/download-artifact@v4
      with:
        pattern: test-results-*
        merge-multiple: true
    
    - name: Create coverage badge
      run: |
        # Extract coverage percentage from XML if exists
        if [ -f coverage-combined.xml ]; then
          COVERAGE=$(python -c "import xml.etree.ElementTree as ET; tree = ET.parse('coverage-combined.xml'); root = tree.getroot(); coverage = float(root.attrib.get('line-rate', 0)) * 100; print(f'{coverage:.1f}')")
          echo "Coverage: ${COVERAGE}%"
          
          # Create a simple JSON for the badge
          echo "{\"coverage\": \"${COVERAGE}%\"}" > coverage-badge.json
        fi
    
    - name: Upload coverage summary
      uses: actions/upload-artifact@v4
      with:
        name: coverage-report-main
        path: |
          coverage-combined.xml
          coverage-badge.json
          htmlcov*/
        retention-days: 90