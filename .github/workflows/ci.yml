name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run nightly to catch flaky tests
    - cron: '0 2 * * *'

# Cancel in-progress runs when a new commit is pushed
concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: '3.12'
  USE_MOCK_SERVER: 'true'
  MOCK_SERVER_PORT: '5000'
  PYTEST_RERUN_FAILURES: '3'  # Retry flaky tests

jobs:
  lint:
    name: Lint and Type Check
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332  # v4.1.7

    - name: Set up Python
      uses: actions/setup-python@82c7e631bb3cdc910f68e0081d67478d79c6982d  # v5.1.0
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Cache dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install ruff mypy

    - name: Run ruff
      run: ruff check .

    - name: Run mypy
      run: mypy libs --ignore-missing-imports

  test:
    name: Test Suite - ${{ matrix.test-type }}
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        test-type: [unit, contracts, comprehensive]

    steps:
    # Skip comprehensive tests unless on main branch or tagged with [full-test]
    - uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332  # v4.1.7
      if: matrix.test-type != 'comprehensive' || github.ref == 'refs/heads/main' || contains(github.event.head_commit.message, '[full-test]')

    - name: Set up Python
      if: matrix.test-type != 'comprehensive' || github.ref == 'refs/heads/main' || contains(github.event.head_commit.message, '[full-test]')
      uses: actions/setup-python@82c7e631bb3cdc910f68e0081d67478d79c6982d  # v5.1.0
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Cache dependencies
      if: matrix.test-type != 'comprehensive' || github.ref == 'refs/heads/main' || contains(github.event.head_commit.message, '[full-test]')
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: Linux-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          Linux-pip-

    - name: Install dependencies
      if: matrix.test-type != 'comprehensive' || github.ref == 'refs/heads/main' || contains(github.event.head_commit.message, '[full-test]')
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-xdist pytest-cov pytest-html pytest-rerunfailures pytest-timeout

    - name: Start Mock Server
      # Only for contracts (integration & comprehensive use Docker)
      if: (matrix.test-type == 'contracts') || (matrix.test-type == 'comprehensive' && (github.ref == 'refs/heads/main' || contains(github.event.head_commit.message, '[full-test]')))
      run: |
        # Start mock server in background
        python -m mock_server.run_server &
        MOCK_PID=$!
        echo "Mock server started with PID: $MOCK_PID"

        # Wait for server to be ready (up to 30 seconds)
        for i in {1..30}; do
          if curl -s --max-time 2 http://localhost:5000/health > /dev/null 2>&1; then
            echo "✓ Mock server is ready with rate limit 500 req/sec"
            break
          fi
          if [[ $i -eq 30 ]]; then
            echo "✗ Mock server failed to start in 30 seconds"
            exit 1
          fi
          echo "Waiting for mock server... ($i/30)"
          sleep 1
        done
      env:
        MOCK_SERVER_RATE_LIMIT: 500
        FLASK_ENV: production

    - name: Run Tests with Coverage
      if: matrix.test-type != 'comprehensive' || github.ref == 'refs/heads/main' || contains(github.event.head_commit.message, '[full-test]')
      run: |
        # coverage 파일명을 명시적으로 지정
        export COVERAGE_FILE=.coverage.${{ matrix.test-type }}

        # 단위 테스트는 병렬 실행 (auto) + 60초 타임아웃
        # 계약 테스트는 순차 실행 + 300초 타임아웃
        # 통합 테스트는 integration-tests-service.yml에서 별도 실행
        if [[ "${{ matrix.test-type }}" = "unit" ]]; then
          # Unit tests run directly (faster, no mock server needed)
          pytest tests/${{ matrix.test-type }}/ -v --tb=short \
            -n auto \
            --timeout=60 \
            --cov=libs --cov-report=term-missing \
            --cov-report=xml:coverage-${{ matrix.test-type }}.xml \
            --junit-xml=test-results-${{ matrix.test-type }}.xml
        elif [[ "${{ matrix.test-type }}" = "contracts" ]]; then
          # Contracts run with Pact v3 (stable, only version)
          pytest tests/${{ matrix.test-type }}/ -v --tb=short \
            --timeout=300 \
            --cov=libs --cov-report=term-missing \
            --cov-report=xml:coverage-${{ matrix.test-type }}.xml \
            --junit-xml=test-results-${{ matrix.test-type }}.xml \
            --use-mock
        elif [[ "${{ matrix.test-type }}" = "comprehensive" ]]; then
          # Build and run Mock Server as separate container
          echo "Building Mock Server Docker image..."
          docker build -f Dockerfile.mock -t billing-mock-server:ci .

          echo "Starting Mock Server..."
          docker run -d \
            --name mock-server-ci \
            --network host \
            -p 5000:5000 \
            -e FLASK_ENV=production \
            -e FLASK_APP=mock_server.app \
            -e PYTHONUNBUFFERED=1 \
            -e MOCK_SERVER_RATE_LIMIT=500 \
            billing-mock-server:ci

          echo "Waiting for Mock Server..."
          for i in {1..60}; do
            if curl -f http://localhost:5000/health 2>/dev/null; then
              echo "✓ Mock server ready"
              break
            fi
            [ $i -eq 60 ] && { echo "✗ Failed to start"; docker logs mock-server-ci; exit 1; }
            sleep 1
          done

          echo "Running Comprehensive Tests..."
          pytest tests/integration/test_all_business_combinations.py \
            tests/integration/test_comprehensive_business_logic.py \
            -v --tb=short \
            -n 2 \
            --timeout=600 \
            --timeout-method=thread \
            --reruns=3 \
            --reruns-delay=3 \
            --max-worker-restart=20 \
            --dist=loadfile \
            --junit-xml=test-results-${{ matrix.test-type }}.xml \
            --use-mock \
            || { TEST_EXIT_CODE=$?; echo "::warning::Comprehensive tests had failures"; }

          echo "Cleanup..."
          docker logs mock-server-ci | tail -50
          docker stop mock-server-ci || true
          docker rm mock-server-ci || true

          exit ${TEST_EXIT_CODE:-0}
        fi
      env:
        PYTEST_TIMEOUT: 600
        COMPOSE_HTTP_TIMEOUT: 600
        USE_MOCK_SERVER: 'true'
        MOCK_SERVER_URL: http://localhost:5000
        MOCK_SERVER_PORT: '5000'
        PYTHONUNBUFFERED: '1'

    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@65462800fd760344b1a7b4382951275a0abb4808  # v4.3.3
      with:
        name: test-results-${{ matrix.test-type }}
        path: |
          test-results-${{ matrix.test-type }}.xml
          coverage-${{ matrix.test-type }}.xml
          .coverage.${{ matrix.test-type }}
          htmlcov/

  performance:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332  # v4.1.7

    - name: Set up Python
      uses: actions/setup-python@82c7e631bb3cdc910f68e0081d67478d79c6982d  # v5.1.0
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-mock.txt
        pip install pytest-benchmark

    - name: Start Mock Server
      run: |
        python -m mock_server.run_server &
        for i in {1..10}; do
          if curl -s http://localhost:5000/health; then
            echo "Mock server is ready with rate limit 500 req/sec"
            break
          fi
          echo "Waiting for mock server..."
          sleep 1
        done
      env:
        MOCK_SERVER_RATE_LIMIT: 500

    - name: Run Performance Tests
      run: |
        pytest tests/performance/ -v --tb=short \
          --junit-xml=test-results-performance.xml
      env:
        CI: true
        MOCK_SERVER_PORT: 5000
        MOCK_SERVER_RATE_LIMIT: 500

    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@65462800fd760344b1a7b4382951275a0abb4808  # v4.3.3
      with:
        name: test-results-performance
        path: |
          test-results-performance.xml

  security:
    name: Security Tests
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332  # v4.1.7

    - name: Set up Python
      uses: actions/setup-python@82c7e631bb3cdc910f68e0081d67478d79c6982d  # v5.1.0
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-mock.txt

    - name: Start Mock Server
      run: |
        python -m mock_server.run_server &
        for i in {1..10}; do
          if curl -s http://localhost:5000/health; then
            echo "Mock server is ready with rate limit 50 req/sec"
            break
          fi
          echo "Waiting for mock server..."
          sleep 1
        done
      env:
        MOCK_SERVER_RATE_LIMIT: 50

    - name: Run Security Tests
      run: |
        pytest tests/security/ -v --tb=short \
          --junit-xml=test-results-security.xml
      env:
        CI: true
        MOCK_SERVER_PORT: 5000

    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@65462800fd760344b1a7b4382951275a0abb4808  # v4.3.3
      with:
        name: test-results-security
        path: |
          test-results-security.xml

  coverage-check:
    name: Coverage Analysis
    runs-on: ubuntu-latest
    needs: [test, integration-tests, component-tests]
    steps:
    - uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332  # v4.1.7

    - name: Set up Python
      uses: actions/setup-python@82c7e631bb3cdc910f68e0081d67478d79c6982d  # v5.1.0
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Download coverage reports
      uses: actions/download-artifact@fa0a91b85d4f404e444e00e005971372dc801d16  # v4.1.7
      with:
        pattern: test-results-*
        merge-multiple: true

    - name: Install coverage
      run: pip install coverage

    - name: Combine coverage reports
      run: |
        # Debug: 다운로드된 파일 확인
        echo "Downloaded files:"
        ls -la

        # coverage 파일 찾기 및 combine
        if ls .coverage.* 1> /dev/null 2>&1; then
          coverage combine .coverage.*
          coverage report --fail-under=80
          coverage html -d htmlcov-combined
          coverage xml -o coverage-combined.xml
        else
          echo "No .coverage.* files found. Checking for xml files..."
          # XML 파일들이 있다면 직접 병합
          if ls coverage-*.xml 1> /dev/null 2>&1; then
            echo "Found coverage XML files, but cannot combine them directly."
            echo "Skipping coverage combine step."
            exit 0
          else
            echo "No coverage files found!"
            exit 1
          fi
        fi

    - name: Upload to Codecov
      # Using specific commit SHA for security (codecov-action v5.0.7)
      uses: codecov/codecov-action@5c47607acb93fed5485fdbf7232e8a31425f672a  # v5.0.7
      with:
        files: ./coverage-combined.xml
        flags: unittests,contracts
        name: combined-coverage
        fail_ci_if_error: true

  publish-coverage:
    name: Publish Coverage Report
    runs-on: ubuntu-latest
    needs: [lint, test, coverage-check]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    steps:
    - uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332  # v4.1.7

    - name: Download coverage reports
      uses: actions/download-artifact@fa0a91b85d4f404e444e00e005971372dc801d16  # v4.1.7
      with:
        pattern: test-results-*
        merge-multiple: true

    - name: Create coverage badge
      run: |
        # Extract coverage percentage from XML if exists
        if [[ -f coverage-combined.xml ]]; then
          COVERAGE=$(python -c "import xml.etree.ElementTree as ET; tree = ET.parse('coverage-combined.xml'); root = tree.getroot(); coverage = float(root.attrib.get('line-rate', 0)) * 100; print(f'{coverage:.1f}')")
          echo "Coverage: ${COVERAGE}%"

          # Create a simple JSON for the badge
          echo "{\"coverage\": \"${COVERAGE}%\"}" > coverage-badge.json
        fi

    - name: Upload coverage summary
      uses: actions/upload-artifact@65462800fd760344b1a7b4382951275a0abb4808  # v4.3.3
      with:
        name: coverage-report-main
        path: |
          coverage-combined.xml
          coverage-badge.json
          htmlcov*/
        retention-days: 90

  integration-tests:
    name: Integration Tests with Mock Server
    runs-on: ubuntu-latest
    needs: [test]
    timeout-minutes: 30

    steps:
      - name: Checkout code
        uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332  # v4.1.7

      - name: Set up Python
        uses: actions/setup-python@82c7e631bb3cdc910f68e0081d67478d79c6982d  # v5.1.0
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest-xdist pytest-timeout pytest-rerunfailures pytest-cov

      - name: Build Mock Server Docker Image
        run: |
          docker build -f Dockerfile.mock -t mock-server:latest .

      - name: Start Mock Server
        run: |
          # Run mock server in background
          docker run -d \
            --name mock-server \
            --network host \
            -p 5000:5000 \
            -e FLASK_ENV=production \
            -e FLASK_APP=mock_server.app \
            -e PYTHONUNBUFFERED=1 \
            -e MOCK_SERVER_RATE_LIMIT=500 \
            mock-server:latest

          echo "Mock server container started"

      - name: Wait for Mock Server
        run: |
          echo "Waiting for mock server to be ready..."
          for i in {1..60}; do
            if curl -f http://localhost:5000/health 2>/dev/null; then
              echo "✓ Mock server is ready after ${i} seconds"
              break
            fi
            if [[ $i -eq 60 ]]; then
              echo "✗ Mock server failed to start in 60 seconds"
              docker logs mock-server
              exit 1
            fi
            echo "Waiting for mock server... ($i/60)"
            sleep 1
          done

      - name: Verify Mock Server
        run: |
          echo "Mock server health check:"
          curl -v http://localhost:5000/health

          echo -e "\nMock server info:"
          curl -v http://localhost:5000/api/v1/info 2>&1 | grep -E "(< HTTP|rate limit)" || true

      - name: Run Integration Tests
        run: |
          pytest tests/integration/ \
            -v \
            --tb=short \
            -n 2 \
            --timeout=300 \
            --timeout-method=thread \
            --reruns=3 \
            --reruns-delay=2 \
            --max-worker-restart=15 \
            --dist=loadfile \
            --cov=libs \
            --cov-report=term-missing \
            --cov-report=xml:coverage-integration.xml \
            --junit-xml=test-results-integration.xml \
            --use-mock
        env:
          USE_MOCK_SERVER: 'true'
          MOCK_SERVER_URL: http://localhost:5000
          MOCK_SERVER_PORT: '5000'
          CI: 'true'
          PYTHONUNBUFFERED: '1'

      - name: Show Mock Server Logs (if failed)
        if: failure()
        run: |
          echo "Mock server logs:"
          docker logs mock-server

      - name: Stop Mock Server
        if: always()
        run: |
          docker stop mock-server || true
          docker rm mock-server || true

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@65462800fd760344b1a7b4382951275a0abb4808  # v4.3.3
        with:
          name: test-results-integration
          path: |
            test-results-integration.xml
            coverage-integration.xml
          retention-days: 30

      - name: Upload coverage to Codecov
        if: always()
        uses: codecov/codecov-action@5c47607acb93fed5485fdbf7232e8a31425f672a  # v5.0.7
        with:
          files: ./coverage-integration.xml
          flags: integration
          name: integration-tests
          fail_ci_if_error: false

  component-tests:
    name: Component Tests (responses)
    runs-on: ubuntu-latest
    needs: [test]
    timeout-minutes: 5

    steps:
      - name: Checkout code
        uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332  # v4.1.7

      - name: Set up Python
        uses: actions/setup-python@82c7e631bb3cdc910f68e0081d67478d79c6982d  # v5.1.0
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install responses pytest-cov

      - name: Run Component Tests
        run: |
          pytest tests/integration/mocked/ \
            -v \
            --tb=short \
            -n auto \
            --timeout=60 \
            --cov=libs \
            --cov-report=term-missing \
            --cov-report=xml:coverage-component.xml \
            --junit-xml=test-results-component.xml
        env:
          PYTHONUNBUFFERED: '1'

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@65462800fd760344b1a7b4382951275a0abb4808  # v4.3.3
        with:
          name: test-results-component
          path: |
            test-results-component.xml
            coverage-component.xml
          retention-days: 30
