name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run nightly to catch flaky tests
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.12'
  USE_MOCK_SERVER: 'true'
  MOCK_SERVER_PORT: '5000'
  PYTEST_RERUN_FAILURES: '3'  # Retry flaky tests

jobs:
  lint:
    name: Lint and Type Check
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install ruff mypy
    
    - name: Run ruff
      run: ruff check .
    
    - name: Run mypy
      run: mypy libs --ignore-missing-imports

  test:
    name: Test Suite - ${{ matrix.test-type }}
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        test-type: [unit, integration, contracts]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: Linux-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          Linux-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-xdist pytest-cov pytest-html pytest-rerunfailures pytest-timeout
    
    - name: Start Mock Server
      if: matrix.test-type != 'unit'
      run: |
        python -m mock_server.run_server &
        for i in {1..10}; do
          if curl -s http://localhost:5000/health; then
            echo "Mock server is ready"
            break
          fi
          echo "Waiting for mock server..."
          sleep 1
        done
    
    - name: Run Tests with Coverage
      run: |
        # coverage íŒŒì¼ëª…ì„ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •
        export COVERAGE_FILE=.coverage.${{ matrix.test-type }}
        if [ "${{ matrix.test-type }}" = "contracts" ]; then
          pytest tests/${{ matrix.test-type }}/ -v --tb=short \
            --cov=libs --cov-report=term-missing \
            --cov-report=xml:coverage-${{ matrix.test-type }}.xml \
            --junit-xml=test-results-${{ matrix.test-type }}.xml \
            --use-mock
        else
          pytest tests/${{ matrix.test-type }}/ -v --tb=short \
            --cov=libs --cov-report=term-missing \
            --cov-report=xml:coverage-${{ matrix.test-type }}.xml \
            --junit-xml=test-results-${{ matrix.test-type }}.xml
        fi
      env:
        PYTEST_TIMEOUT: 300
    
    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: test-results-${{ matrix.test-type }}
        path: |
          test-results-${{ matrix.test-type }}.xml
          coverage-${{ matrix.test-type }}.xml
          .coverage.${{ matrix.test-type }}
          htmlcov/

  performance:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-benchmark
    
    - name: Run Performance Tests
      run: |
        pytest tests/performance/ \
          --benchmark-only \
          --benchmark-json=benchmark.json \
          --benchmark-autosave
    
    - name: Store benchmark result as artifact
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results
        path: benchmark.json
        retention-days: 30
    
    - name: Comment benchmark result
      if: github.event_name == 'pull_request'
      uses: benchmark-action/github-action-benchmark@v1
      with:
        tool: 'pytest'
        output-file-path: benchmark.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        # PR ì½”ë©˜íŠ¸ë¡œë§Œ ê²°ê³¼ í‘œì‹œ (gh-pages ì‚¬ìš© ì•ˆ í•¨)
        comment-always: true
        save-data-file: false
        alert-threshold: '150%'
        comment-on-alert: true
        fail-on-alert: false
    
    - name: Add benchmark to job summary
      if: always()
      run: |
        echo "## ðŸ“Š Performance Benchmark Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        if [ -f benchmark.json ]; then
          python -c "import json; f = open('benchmark.json'); data = json.load(f); f.close(); [print(f\"- **{benchmark['name']}**: {benchmark['stats']['mean']:.4f}s Â± {benchmark['stats']['stddev']:.4f}s\") for benchmark in data.get('benchmarks', [])]" >> $GITHUB_STEP_SUMMARY
        else
          echo "No benchmark results found." >> $GITHUB_STEP_SUMMARY
        fi

  flaky-test-detection:
    name: Flaky Test Detection
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-xdist pytest-rerunfailures pytest-json-report
    
    - name: Run tests multiple times
      run: |
        for i in {1..5}; do
          echo "Run $i of 5"
          pytest tests/ \
            -n auto \
            --json-report \
            --json-report-file=report-$i.json \
            || true
        done
    
    - name: Analyze flaky tests
      run: |
        python tests/analyze_flaky_tests.py report-*.json > flaky-tests-report.md
    
    - name: Upload flaky test report
      uses: actions/upload-artifact@v4
      with:
        name: flaky-tests-report
        path: flaky-tests-report.md
    
    - name: Create issue for flaky tests
      if: failure()
      uses: actions/github-script@v8
      with:
        script: |
          const fs = require('fs');
          const report = fs.readFileSync('flaky-tests-report.md', 'utf8');
          await github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: 'Flaky Tests Detected',
            body: report,
            labels: ['flaky-test', 'automated']
          });

  coverage-check:
    name: Coverage Analysis
    runs-on: ubuntu-latest
    needs: test
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Download coverage reports
      uses: actions/download-artifact@v4
      with:
        pattern: test-results-*
        merge-multiple: true
    
    - name: Install coverage
      run: pip install coverage
    
    - name: Combine coverage reports
      run: |
        # Debug: ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ í™•ì¸
        echo "Downloaded files:"
        ls -la
        
        # coverage íŒŒì¼ ì°¾ê¸° ë° combine
        if ls .coverage.* 1> /dev/null 2>&1; then
          coverage combine .coverage.*
          coverage report --fail-under=80
          coverage html -d htmlcov-combined
          coverage xml -o coverage-combined.xml
        else
          echo "No .coverage.* files found. Checking for xml files..."
          # XML íŒŒì¼ë“¤ì´ ìžˆë‹¤ë©´ ì§ì ‘ ë³‘í•©
          if ls coverage-*.xml 1> /dev/null 2>&1; then
            echo "Found coverage XML files, but cannot combine them directly."
            echo "Skipping coverage combine step."
            exit 0
          else
            echo "No coverage files found!"
            exit 1
          fi
        fi
    
    - name: Upload to Codecov
      uses: codecov/codecov-action@v4
      with:
        file: ./coverage-combined.xml
        flags: unittests,integration,contracts
        name: combined-coverage
        fail_ci_if_error: true

  all-tests-450:
    name: Full 450 Combination Test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' || contains(github.event.head_commit.message, '[full-test]')
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-xdist
    
    - name: Start Mock Server
      run: |
        python -m mock_server.run_server &
        sleep 5
    
    - name: Run all 450 combinations
      run: |
        python tests/run_all_tests.py --suite 450
      timeout-minutes: 30
    
    - name: Upload results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: 450-combinations-results
        path: |
          test-results-*.xml
          htmlcov/

  publish-coverage:
    name: Publish Coverage Report
    runs-on: ubuntu-latest
    needs: [lint, test, coverage-check]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    steps:
    - uses: actions/checkout@v4
    
    - name: Download coverage reports
      uses: actions/download-artifact@v4
      with:
        pattern: test-results-*
        merge-multiple: true
    
    - name: Create coverage badge
      run: |
        # Extract coverage percentage from XML if exists
        if [ -f coverage-combined.xml ]; then
          COVERAGE=$(python -c "import xml.etree.ElementTree as ET; tree = ET.parse('coverage-combined.xml'); root = tree.getroot(); coverage = float(root.attrib.get('line-rate', 0)) * 100; print(f'{coverage:.1f}')")
          echo "Coverage: ${COVERAGE}%"
          
          # Create a simple JSON for the badge
          echo "{\"coverage\": \"${COVERAGE}%\"}" > coverage-badge.json
        fi
    
    - name: Upload coverage summary
      uses: actions/upload-artifact@v4
      with:
        name: coverage-report-main
        path: |
          coverage-combined.xml
          coverage-badge.json
          htmlcov*/
        retention-days: 90