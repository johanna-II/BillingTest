name: CI/CD Pipeline - Improved

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run nightly to catch flaky tests
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.12'
  USE_MOCK_SERVER: 'true'
  MOCK_SERVER_PORT: '5000'
  PYTEST_RERUN_FAILURES: '3'  # Retry flaky tests

jobs:
  lint:
    name: Lint and Type Check
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install ruff mypy
    
    - name: Run ruff
      run: ruff check .
    
    - name: Run mypy
      run: mypy libs --ignore-missing-imports

  test:
    name: Test Suite - ${{ matrix.test-type }} on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest]
        test-type: [unit, integration, contract]
        exclude:
          # Skip contract tests on Windows for now
          - os: windows-latest
            test-type: contract
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache dependencies
      uses: actions/cache@v4
      with:
        path: |
          ~/.cache/pip
          ~\AppData\Local\pip\Cache
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-xdist pytest-cov pytest-html pytest-rerunfailures pytest-timeout
    
    - name: Start Mock Server (Linux)
      if: matrix.test-type != 'unit' && runner.os == 'Linux'
      run: |
        python -m mock_server.run_server &
        for i in {1..10}; do
          if curl -s http://localhost:5000/health; then
            echo "Mock server is ready"
            break
          fi
          echo "Waiting for mock server..."
          sleep 1
        done
    
    - name: Start Mock Server (Windows)
      if: matrix.test-type != 'unit' && runner.os == 'Windows'
      shell: pwsh
      run: |
        Start-Process python -ArgumentList "-m", "mock_server.run_server" -NoNewWindow
        for ($i = 1; $i -le 10; $i++) {
          try {
            $response = Invoke-WebRequest -Uri http://localhost:5000/health -UseBasicParsing
            if ($response.StatusCode -eq 200) {
              Write-Host "Mock server is ready"
              break
            }
          } catch {
            Write-Host "Waiting for mock server..."
            Start-Sleep -Seconds 1
          }
        }
    
    - name: Run Tests with Retry
      run: |
        python tests/run_all_tests.py --suite ${{ matrix.test-type }} --no-parallel
      env:
        PYTEST_TIMEOUT: 300
    
    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: test-results-${{ matrix.os }}-${{ matrix.test-type }}
        path: |
          test-results-*.xml
          coverage.xml
          htmlcov/
          .coverage*

  performance:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-benchmark
    
    - name: Run Performance Tests
      run: |
        pytest tests/performance/ \
          --benchmark-only \
          --benchmark-json=benchmark.json \
          --benchmark-autosave
    
    - name: Store benchmark result
      uses: benchmark-action/github-action-benchmark@v1
      with:
        tool: 'pytest'
        output-file-path: benchmark.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true
        alert-threshold: '150%'
        comment-on-alert: true
        fail-on-alert: false

  flaky-test-detection:
    name: Flaky Test Detection
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-xdist pytest-rerunfailures pytest-json-report
    
    - name: Run tests multiple times
      run: |
        for i in {1..5}; do
          echo "Run $i of 5"
          pytest tests/ \
            -n auto \
            --json-report \
            --json-report-file=report-$i.json \
            || true
        done
    
    - name: Analyze flaky tests
      run: |
        python tests/analyze_flaky_tests.py report-*.json > flaky-tests-report.md
    
    - name: Upload flaky test report
      uses: actions/upload-artifact@v4
      with:
        name: flaky-tests-report
        path: flaky-tests-report.md
    
    - name: Create issue for flaky tests
      if: failure()
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const report = fs.readFileSync('flaky-tests-report.md', 'utf8');
          await github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: 'Flaky Tests Detected',
            body: report,
            labels: ['flaky-test', 'automated']
          });

  coverage-check:
    name: Coverage Analysis
    runs-on: ubuntu-latest
    needs: test
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Download coverage reports
      uses: actions/download-artifact@v4
      with:
        pattern: test-results-*
        merge-multiple: true
    
    - name: Install coverage
      run: pip install coverage
    
    - name: Combine coverage reports
      run: |
        coverage combine .coverage.*
        coverage report --fail-under=80
        coverage html -d htmlcov-combined
        coverage xml -o coverage-combined.xml
    
    - name: Upload to Codecov
      uses: codecov/codecov-action@v4
      with:
        file: ./coverage-combined.xml
        flags: unittests,integration,contracts
        name: combined-coverage
        fail_ci_if_error: true

  all-tests-450:
    name: Full 450 Combination Test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' || contains(github.event.head_commit.message, '[full-test]')
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-xdist
    
    - name: Start Mock Server
      run: |
        python -m mock_server.run_server &
        sleep 5
    
    - name: Run all 450 combinations
      run: |
        python tests/run_all_tests.py --suite 450
      timeout-minutes: 30
    
    - name: Upload results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: 450-combinations-results
        path: |
          test-results-*.xml
          htmlcov/
